---
title: "NLP"
author: "Akash"
date: '2022-06-01'
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}

```

```{r}
packages = c(
  'rtweet',
  'httpuv',
  'tidyverse',
  'rtweet',
  'tidytext',
  'ggwordcloud',
  'reshape2',
  'wordcloud',
  'igraph',
  'ggraph',
  'topicmodels',
  'tm'
)

package.check <- lapply( #by vikram
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
    }
  }
)
```

```{r}
library('rtweet')
library('httpuv')

api_key <- "8GwQcGEWxLAbcHvJBbdlqu7Xf"
api_secret_key <- "tzsZNg9rvDNqrCA3Btc4Gka9iv6W9NkSxo2WfSwsGZIHm8Rnk3"
app_name <- "CenterScrape"
access_token <- "635181580-sD7F6vWdH7gt8kPHh4i90HolDX4aWZNhFZ8Y9DV7"
access_token_secret<-"ZmtuRto1MYnSuUwkPeANh8MVkFcjCtA4YMv0MNooZUhvE"

token <- create_token(
  app = app_name,
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret
)

get_token()
```
```{r}
library('tidyverse')
library('rtweet')
timelineDF <- get_timelines("lifehacker")

removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)

tweet <- timelineDF %>%
  filter(is_retweet == FALSE) %>% 
  select(text)%>%
  cbind(tweet_id = 58:1)%>%
  rename(tweet = text)%>%
  mutate(tweet = removeURL(tweet))

```

```{r}
library('tidytext')
tokenized_tweets <- unnest_tokens(tweet, input = 'tweet', output = 'word')
head(tokenized_tweets)
tokenized_tweets %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
    geom_col()  + 
    labs(title = "Count of Words in CCIDM Tweets") + 
    scale_x_continuous(breaks = seq(0, 50, 5))

```
```{r}
tokenized_tweets %>%
  anti_join(stop_words) %>% #finds where tweet words overlap with predefined stop words, and removes them
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  filter(count > 5) %>%
  mutate(word = reorder(word, count)) %>%
  ggplot(aes(x = count, y = word)) + 
    geom_col() + 
    labs(title = "Count of Words in CCIDM Tweets") + 
    scale_x_continuous(breaks = seq(0, 50, 5))

```
```{r}
library('ggwordcloud')

tokenized_tweets %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(n > 4) %>%
  ggplot(aes(label = word, size = n, color = n)) + 
    geom_text_wordcloud() + 
    scale_size_area(max_size = 15) 
```
```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```
```{r}
tokenized_tweets %>%
  group_by(tweet_id) %>%
  inner_join(get_sentiments("afinn")) %>%
  summarise(mean_sentiment = mean(value)) %>%
  ggplot(aes(x = tweet_id, y = mean_sentiment)) + 
    geom_col() + 
    labs(title = 'Mean Sentiment by Tweet - Afinn Lexicon', x = "Tweet ID", y = 'Mean Sentiment') + 
    scale_x_continuous(breaks = seq(1, 50)) +
    scale_y_continuous(breaks = seq(-1, 3, 0.5))
```

```{r}
library('reshape2')
library('wordcloud')

tokenized_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% #cast into matrix, grouped by neg and pos
  comparison.cloud(colors = c("red", "green"),
                   max.words = 20)
```
```{r}
tokenized_tweets %>%
  count(word, sort = TRUE) %>%
  rename(count = n) %>%
  mutate(total=sum(count))%>%
  mutate(tf=count/total) %>%
  head()

tweet_td_idf <- tokenized_tweets %>%
  count(word, tweet_id, sort = TRUE) %>%
  rename(count = n) %>%
  bind_tf_idf(word, tweet_id, count)

tweet_td_idf %>%
  select(word, tweet_id, tf_idf, count)%>%
  group_by(tweet_id)%>%
  slice_max(order_by = tf_idf, n=6, with_ties = FALSE)%>%
  filter(tweet_id < 6)%>%
  ggplot(aes(label = word))+
  geom_text_wordcloud()+
  facet_grid(rows = vars(tweet_id))

```

```{r}
tweets_bigram <- tweet %>%
  unnest_tokens(bigram, tweet, token = 'ngrams', n = 2)


head(tweets_bigram)
 
  

tweets_bigram <- tweets_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- tweets_bigram %>%
  count(word1, word2, sort = TRUE)
```


```{r}
tweet %>%
  unnest_tokens(bigram, tweet, token = 'ngrams', n = 2) %>%
  count(tweet_id, bigram) %>%
  bind_tf_idf(bigram, tweet_id, n) %>%
  group_by(tweet_id) %>%
  arrange(tweet_id, desc(tf_idf)) %>%
  head()



```

```{r}
library('igraph')
library('ggraph')
bi_graph <- bigram_counts  %>%
  filter(n > 2) %>% 
  graph_from_data_frame()

ggraph(bi_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

```{r}
tweets_trigram <- tweet %>%
  unnest_tokens(trigram, tweet, token = 'ngrams', n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% #separates on whitespace
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(!word3 %in% stop_words$word)

trigram_counts <- tweets_trigram %>%
  count(word1, word2, word3, sort = TRUE)
```
```{r}
tweet %>%
  unnest_tokens(trigram, tweet, token = 'ngrams', n = 3) %>%
  count(tweet_id, trigram) %>%
  bind_tf_idf(trigram, tweet_id, n) %>%
  group_by(tweet_id) %>%
  arrange(tweet_id, desc(tf_idf)) %>%
  head()
```

```{r}
library('igraph')
library('ggraph')
tri_graph <- trigram_counts %>%
  filter(n > 2) %>% 
  graph_from_data_frame()

ggraph(tri_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


```{r}
library('topicmodels')
library('tm')

#parameters
num_topics=3
top_n_to_get=10

tweets_lda <- tweet_td_idf %>%
  anti_join(stop_words) %>%
  cast_dtm(document = tweet_id, term =  word, value =  count) %>%
  LDA(k=num_topics) 

tweets_lda
```
```{r}

```




